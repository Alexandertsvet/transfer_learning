{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opendatasets in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (0.1.22)\n",
      "Requirement already satisfied: tqdm in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from opendatasets) (4.66.5)\n",
      "Requirement already satisfied: kaggle in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from opendatasets) (1.6.17)\n",
      "Requirement already satisfied: click in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from opendatasets) (8.1.7)\n",
      "Requirement already satisfied: six>=1.10 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (2.32.3)\n",
      "Requirement already satisfied: python-slugify in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (2.2.2)\n",
      "Requirement already satisfied: bleach in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from kaggle->opendatasets) (6.2.0)\n",
      "Requirement already satisfied: webencodings in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexandr/stydy/eda/venv/lib/python3.10/site-packages (from requests->kaggle->opendatasets) (3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./imdb-user-reviews\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "dataset_path = Path('imdb-user-reviews', 'song_lyrics.csv')\n",
    "if not dataset_path.is_file():\n",
    "    od.download('https://www.kaggle.com/datasets/sadmadlad/imdb-user-reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.03 1.051712888577486\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "n, mean, M2 = 0, 0.0, 0\n",
    "for path in Path('imdb-user-reviews').glob('**/*'):\n",
    "    if path.is_file() and path.suffix == '.json':\n",
    "        with open(path, 'r') as f:\n",
    "            info = json.load(f)\n",
    "        score = float(info['movieIMDbRating'])\n",
    "        n += 1\n",
    "        delta = score - mean\n",
    "        mean += delta / n\n",
    "        M2 += delta * (score - mean)\n",
    "\n",
    "print(mean, (M2 / n) ** (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.030000000000001\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for path in Path('imdb-user-reviews').glob('**/*'):\n",
    "    if path.is_file() and path.suffix == '.json':\n",
    "        with open(path, 'r') as f:\n",
    "            info = json.load(f)\n",
    "        score = float(info['movieIMDbRating'])\n",
    "        result.append(score)\n",
    "print(sum(result)/len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    x = x.strip()\n",
    "    if isinstance(x, str):\n",
    "        if len(x)<=2:\n",
    "            return int(x)\n",
    "        else:\n",
    "            return np.nan\n",
    "    if isinstance(x, int) or isinstance(x, float):\n",
    "        return x\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(list_of_strings, number_of_chunks=30):\n",
    "    step = len(list_of_strings) // number_of_chunks\n",
    "    if step != 0:\n",
    "        for i in range(0, len(list_of_strings), step):\n",
    "            yield list_of_strings[i : i + step]\n",
    "    else:\n",
    "        yield list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks_mapper(data):\n",
    "    return (len(data), np.sum(data))\n",
    "\n",
    "def reducer(data_1, data_2):\n",
    "    return (np.sum(data_1[0])+np.sum(data_2[0]), np.sum(data_1[1])+np.sum(data_2[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 290 ms, sys: 50.1 ms, total: 340 ms\n",
      "Wall time: 408 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for path in Path('imdb-user-reviews').glob('*/*.csv'):\n",
    "\n",
    "     with open(path, 'r') as f:\n",
    "          data = pd.read_csv(f)\n",
    "          data = data[\"User's Rating out of 10\"].apply(preprocessing)\n",
    "          data = data[~data.isna()].values\n",
    "          data_chunk = chunkify(data, 30)\n",
    "          mapped = map(chunks_mapper, data_chunk)\n",
    "          data = reduce(reducer, mapped)\n",
    "          result.append(data)       \n",
    "result_sum = np.sum([x[1] for x in result])\n",
    "result_len_sum = np.sum([x[0] for x in result])\n",
    "result = result_sum/result_len_sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cредний балл фильмов:8.08\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cредний балл фильмов:{result:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 374 ms, sys: 90.8 ms, total: 465 ms\n",
      "Wall time: 598 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for path in Path('imdb-user-reviews').glob('*/*.csv'):\n",
    "\n",
    "     with open(path, 'r') as f:\n",
    "          data = pd.read_csv(f)\n",
    "          data = data[\"User's Rating out of 10\"].apply(preprocessing)\n",
    "          data = data[~data.isna()].values\n",
    "          data_chunk = chunkify(data, 30)\n",
    "          mapped = Parallel(n_jobs=5)(delayed(chunks_mapper)(chunk) for chunk in data_chunk)\n",
    "          #mapped = map(chunks_mapper, data_chunk)\n",
    "          data = reduce(reducer, mapped)\n",
    "          result.append(data)\n",
    "result_sum = np.sum([x[1] for x in result])\n",
    "result_len_sum = np.sum([x[0] for x in result])\n",
    "result = result_sum/result_len_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cредний балл фильмов:8.08\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cредний балл фильмов:{result:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 364 ms, sys: 182 ms, total: 546 ms\n",
      "Wall time: 569 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for path in Path('imdb-user-reviews').glob('*/*.csv'):\n",
    "\n",
    "     with open(path, 'r') as f:\n",
    "          data = pd.read_csv(f)\n",
    "          data = data[\"User's Rating out of 10\"].apply(preprocessing)\n",
    "          data = data[~data.isna()].values\n",
    "          data_chunk = chunkify(data, 30)\n",
    "          with Pool(5) as p:\n",
    "            mapped = p.map(chunks_mapper, data_chunk)\n",
    "            data = reduce(reducer, mapped)\n",
    "            result.append(data)\n",
    "result_sum = np.sum([x[1] for x in result])\n",
    "result_len_sum = np.sum([x[0] for x in result])\n",
    "result = result_sum/result_len_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cредний балл фильмов:8.08\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cредний балл фильмов:{result:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# вывод:\n",
    "* последователное выполнение кода показала лучший результат, \\\n",
    "это обусловлдено небольшим объемом данных который возможно поместить в операционную памать,\\\n",
    "что нивелирует использование быблиотек паральльной обработки данных.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
